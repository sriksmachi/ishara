{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy\n",
    "# !pip install Pillow\n",
    "# !pip install imageio\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib\n",
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "import datetime\n",
    "import os\n",
    "import cv2\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.random.set_random_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n",
    "\n",
    "batch_size = 30 #experiment with the batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let us plot some sample images and run some transformations on the image so see the impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Pick some random sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir_train = 'Project_data/train/'\n",
    "random_sequence = train_doc[np.random.randint(len(train_doc))].strip().split(';')[0]+'/'\n",
    "images = os.listdir(data_dir_train + random_sequence)\n",
    "# Create a code to visualize one instance of all the 30 images present in the sequence\n",
    "plt.figure(figsize=(10, 10))\n",
    "i = 0\n",
    "random_images = []\n",
    "for img in images:\n",
    "  ax = plt.subplot(6, 5, i + 1)\n",
    "  i = i + 1\n",
    "  random_images.append(os.path.join(data_dir_train, random_sequence, img))\n",
    "  img_bgr = cv2.imread(os.path.join(data_dir_train, random_sequence, img))\n",
    "  img_mp = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB) # This is needed because opencv uses BGR convention and matplotlib uses RGB\n",
    "  imgplot= plt.imshow(img_mp)\n",
    "  plt.axis(\"off\")\n",
    "  plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Image - Original\n",
    "random_image = random_images[np.random.randint(30)]\n",
    "image_bgr = cv2.imread(random_image)\n",
    "plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "print(image_bgr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cropped Image Sample\n",
    "crop_img = image_bgr[10:120, 10:160]\n",
    "plt.imshow(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))\n",
    "print(crop_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize Image Sample\n",
    "dim = (120, 120)\n",
    "resized_img = cv2.resize(crop_img, dim, interpolation = cv2.INTER_AREA)\n",
    "plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
    "print(resized_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Image\n",
    "plt.imshow(cv2.cvtColor(cv2.normalize(resized_img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crop_image(image, size = 10):\n",
    "     # cropping the image\n",
    "    image_x = image.shape[0]\n",
    "    image_y = image.shape[1]\n",
    "    return image[size:image_x, size:image_y]\n",
    "\n",
    "def normalize_minmax(image_resized, batch_data, folder, idx):\n",
    "    batch_data[folder,idx,:,:,0] = cv2.normalize(image_resized[:,:,0], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    batch_data[folder,idx,:,:,1] = cv2.normalize(image_resized[:,:,1], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    batch_data[folder,idx,:,:,2] = cv2.normalize(image_resized[:,:,2], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    return batch_data   \n",
    "\n",
    "def crop_and_normalize(image,batch_data, folder, idx, y=120,z=120):\n",
    "    \n",
    "    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "    image_cropped = crop_image(image)\n",
    "    image_resized = cv2.resize(image_cropped, (y,z), interpolation = cv2.INTER_AREA)\n",
    "                    \n",
    "    # using min max normalization.\n",
    "    pending_batch_data = normalize_minmax(image_resized, batch_data, folder, idx)\n",
    "    return pending_batch_data\n",
    "\n",
    "def generator(source_path, folder_list, batch_size):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    x = 30 # taking 5 images from each video\n",
    "    img_idx = range(1,x) #create a list of image numbers you want to use for a particular video\n",
    "    y = 120 # image dim\n",
    "    z = 120 # image dim\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches =  len(t) // batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size, 5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                \n",
    "                ## get images from the folder\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                \n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                \n",
    "                    image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    batch_data = crop_and_normalize(image, batch_data, folder, idx)\n",
    "                   \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "                \n",
    "            yield batch_data, batch_labels \n",
    "            \n",
    "        # The length of the folder list could leave some residue folders, the below code deals with it\n",
    "        # Number of pending batches\n",
    "        pending_batches = len(t) % batch_size\n",
    "        \n",
    "        pending_batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "        pending_batch_labels = np.zeros((batch_size, 5)) # batch_labels is the one hot representation of the output\n",
    "        \n",
    "        for folder in range(pending_batches): # iterate over the batch_size\n",
    "\n",
    "            # getimages from the folder\n",
    "            imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                    \n",
    "            for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    \n",
    "                image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                # using min max normalization.\n",
    "                pending_batch_data = crop_and_normalize(image, pending_batch_data, folder, idx)\n",
    "                    \n",
    "            pending_batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "        yield pending_batch_data, pending_batch_labels\n",
    "            \n",
    "\n",
    "# g = generator(data_dir_train, train_doc, 30)\n",
    "# next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = generator(data_dir_train, train_doc, 30)\n",
    "# p = 0\n",
    "# for k in g:\n",
    "#     p+=1\n",
    "# print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 1\n"
     ]
    }
   ],
   "source": [
    "train_path = 'Project_data/train'\n",
    "val_path = 'Project_data/val'\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "num_epochs = 1 # choose the number of epochs\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "\n",
    "#write your model here\n",
    "\n",
    "# Building a 3 D Convolution model.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(30,120,120,3)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Conv3D(64, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_9 (Conv3D)            (None, 28, 118, 118, 32)  2624      \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 14, 59, 59, 32)    0         \n",
      "_________________________________________________________________\n",
      "conv3d_10 (Conv3D)           (None, 12, 57, 57, 64)    55360     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling (None, 6, 28, 28, 64)     0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 301056)            0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 256)               77070592  \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 77,195,653\n",
      "Trainable params: 77,195,653\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "optimiser =  'sgd' #write your optimizer\n",
    "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Here we define few callbacks which we will use later when we fit the complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0423 18:34:33.455706 140388283520832 callbacks.py:1071] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    \n",
    "if not os.path.exists(model_name):\n",
    "    os.mkdir(model_name)\n",
    "        \n",
    "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "#LR = # write the REducelronplateau code here\n",
    "callbacks_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch: 23, validation_steps: 4\n"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "print('steps_per_epoch: {}, validation_steps: {}'.format(steps_per_epoch, validation_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Experiment\n",
    "\n",
    "Let us now fit the model with 1 epoch just to validate if the model is working.\n",
    "Notice I have not added any callbacks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 30\n",
      "23/23 [==============================] - ETA: 0s - loss: 2.6429 - categorical_accuracy: 0.2101Source path =  Project_data/val ; batch size = 30\n",
      "23/23 [==============================] - 61s 3s/step - loss: 2.6429 - categorical_accuracy: 0.2101 - val_loss: 1.2796 - val_categorical_accuracy: 0.1750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fada02cad30>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                    callbacks=[], validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to be working well, there are no errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on training data. \n",
    "\n",
    "Let us overfit on the training data to see if the model is able to learn from the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 30\n",
    "train_generator = generator(train_path, train_doc, batch_size)\n",
    "val_generator = generator(val_path, val_doc, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch: 23, validation_steps: 4\n"
     ]
    }
   ],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1\n",
    "\n",
    "print('steps_per_epoch: {}, validation_steps: {}'.format(steps_per_epoch, validation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  Project_data/train ; batch size = 30\n",
      "Epoch 1/10\n",
      "23/23 [==============================] - ETA: 0s - loss: 1.3581 - categorical_accuracy: 0.3870Source path =  Project_data/val ; batch size = 30\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.3581 - categorical_accuracy: 0.3870 - val_loss: 1.2029 - val_categorical_accuracy: 0.2833\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.1970 - categorical_accuracy: 0.4754 - val_loss: 0.9375 - val_categorical_accuracy: 0.4250\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 61s 3s/step - loss: 1.1161 - categorical_accuracy: 0.5333 - val_loss: 1.2791 - val_categorical_accuracy: 0.3667\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 59s 3s/step - loss: 0.9894 - categorical_accuracy: 0.5797 - val_loss: 0.8876 - val_categorical_accuracy: 0.4667\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.6823 - categorical_accuracy: 0.7130 - val_loss: 0.9129 - val_categorical_accuracy: 0.4750\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.5775 - categorical_accuracy: 0.7638 - val_loss: 0.7886 - val_categorical_accuracy: 0.5750\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.3735 - categorical_accuracy: 0.8536 - val_loss: 0.7221 - val_categorical_accuracy: 0.5250\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 58s 3s/step - loss: 0.2900 - categorical_accuracy: 0.8710 - val_loss: 0.8400 - val_categorical_accuracy: 0.5500\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 60s 3s/step - loss: 0.1505 - categorical_accuracy: 0.9333 - val_loss: 0.9930 - val_categorical_accuracy: 0.5167\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 61s 3s/step - loss: 0.0775 - categorical_accuracy: 0.9565 - val_loss: 0.8763 - val_categorical_accuracy: 0.5917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fada01aca90>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=10, verbose=1, \n",
    "                    callbacks=[], validation_data=val_generator, \n",
    "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
