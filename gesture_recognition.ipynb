{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Neural_Nets_Project_Starter_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.6 - AzureML",
      "language": "python",
      "name": "python3-azureml"
    },
    "metadata": {
      "interpreter": {
        "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sriksmachi/ishara/blob/main/gesture_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiusepyhRj9G"
      },
      "source": [
        "# Gesture Recognition\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reKOec6PRj9I"
      },
      "source": [
        "# !pip install scipy\n",
        "# !pip install Pillow\n",
        "# !pip install imageio\n",
        "# !pip install tensorflow\n",
        "# !pip install matplotlib\n",
        "#!pip install opencv-python\n",
        "# !pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAyw0okCRj9J"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import imageio\n",
        "import datetime\n",
        "import os\n",
        "import cv2\n",
        "import pathlib\n",
        "import gdown\n",
        "from scipy import misc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-F6TTavRj9K"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7NjsiqBRj9K"
      },
      "source": [
        "import random as rn\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "rn.seed(30)\n",
        "np.random.seed(30)\n",
        "tf.compat.v1.random.set_random_seed(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9D0Nh7mJbOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f2c8d6-eda3-48a9-e90a-2a25d5e9ed6a"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwxmRFbIJ2Ex",
        "outputId": "f34305fe-0220-4219-c03d-04ff211b17c4"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiDiFQCNJ8qh",
        "outputId": "ee1784fc-5fe1-4f2f-85f3-a878b73ee254"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Apr 26 10:31:38 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGHoh_fZgszy"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSDefjBaQjF9",
        "outputId": "f038e4ef-b6a4-4301-c798-d96d36265dcc"
      },
      "source": [
        "!gdown 'https://drive.google.com/uc?export=download&id=1kl5iauN3iVCPnSHys0xvTUnAd41CNu3S'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1kl5iauN3iVCPnSHys0xvTUnAd41CNu3S\n",
            "To: /content/Project_data.zip\n",
            "740MB [00:05, 158MB/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/gdown/cli.py\", line 61, in main\n",
            "    quiet=args.quiet,\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/gdown/download.py\", line 101, in download\n",
            "    for chunk in res.iter_content(chunk_size=CHUNK_SIZE):\n",
            "\n",
            "746MB [00:05, 140MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUPUsSDvdqLB",
        "outputId": "d1e9a4b2-e7ba-42bb-a4ba-30ea81e9a8ee"
      },
      "source": [
        "!unzip /content/Project_data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/Project_data.zip\n",
            "replace Project_data/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dj6P85xRj9L"
      },
      "source": [
        "## Use this to load the data if the data exists on gdrive and using google colab for \n",
        "\n",
        "data_dir=\"Project_data/\"\n",
        "data_dir_train = \"Project_data/train/\"\n",
        "data_dir_val = 'Project_data/val/'\n",
        "\n",
        "train_doc = np.random.permutation(open('Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('Project_data/val.csv').readlines())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb6UeUD_Rj9L"
      },
      "source": [
        "# ## Use this to load the data if the data exists on Azure ML\n",
        "# # azureml-core of version 1.0.72 or higher is required\n",
        "# from azureml.core import Workspace, Dataset\n",
        "\n",
        "# subscription_id = '925a5ad8-a21a-48c0-92a2-5a8a2a4dfc46'\n",
        "# resource_group = 'machinelearning-workbench'\n",
        "# workspace_name = 'sriks-azureml'\n",
        "\n",
        "# workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "# dataset = Dataset.get_by_name(workspace, name='gesture_ds')\n",
        "\n",
        "# ## Uncommenting the below will initiate the download, set Overwrite=True if overwriting existing files is intended. \n",
        "# # doc: https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.filedataset?view=azure-ml-py#download-target-path-none--overwrite-false-\n",
        "# dataset.download(target_path='Project_data/', overwrite=False)\n",
        "\n",
        "# train_path = 'Project_data/train'\n",
        "# val_path = 'Project_data/val'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mW_fAmsRj9M"
      },
      "source": [
        "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o90Ro9DYRj9M"
      },
      "source": [
        "\n",
        "train_doc = np.random.permutation(open(data_dir+'train.csv').readlines())\n",
        "val_doc = np.random.permutation(open(data_dir+'val.csv').readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhf5fPi6Rj9N"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Let us plot some sample images and run some transformations on the image so see the impact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yri6PC4Rj9N"
      },
      "source": [
        "# # ## Pick some random sequence\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# random_sequence = train_doc[np.random.randint(len(train_doc))].strip().split(';')[0]+'/'\n",
        "# images = os.listdir(data_dir_train + random_sequence)\n",
        "# # Create a code to visualize one instance of all the 30 images present in the sequence\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# i = 0\n",
        "# random_images = []\n",
        "# for img in images:\n",
        "#   ax = plt.subplot(6, 5, i + 1)\n",
        "#   i = i + 1\n",
        "#   random_images.append(os.path.join(data_dir_train, random_sequence, img))\n",
        "#   img_bgr = cv2.imread(os.path.join(data_dir_train, random_sequence, img))\n",
        "#   img_mp = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB) # This is needed because opencv uses BGR convention and matplotlib uses RGB\n",
        "#   imgplot= plt.imshow(img_mp)\n",
        "#   plt.axis(\"off\")\n",
        "#   plt.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCdZttJeRj9N"
      },
      "source": [
        "# # Random Image - Original\n",
        "# random_image = random_images[np.random.randint(30)]\n",
        "# image_bgr = cv2.imread(random_image)\n",
        "# plt.imshow(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
        "# print(image_bgr.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzK1SeTyRj9O"
      },
      "source": [
        "# ## Cropped Image Sample\n",
        "# crop_img = image_bgr[10:120, 10:160]\n",
        "# plt.imshow(cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB))\n",
        "# print(crop_img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MDYVM7ERj9O"
      },
      "source": [
        "# # Resize Image Sample\n",
        "# dim = (120, 120)\n",
        "# resized_img = cv2.resize(crop_img, dim, interpolation = cv2.INTER_AREA)\n",
        "# plt.imshow(cv2.cvtColor(resized_img, cv2.COLOR_BGR2RGB))\n",
        "# print(resized_img.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BuN2BS9Rj9O"
      },
      "source": [
        "# # Normalized Image\n",
        "# plt.imshow(cv2.cvtColor(cv2.normalize(resized_img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F), cv2.COLOR_BGR2RGB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HCnC3XxRj9O"
      },
      "source": [
        "## Generator\n",
        "\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hy0jJBiRj9O"
      },
      "source": [
        "def crop_image(image, size = 10):\n",
        "     # cropping the image\n",
        "    image_x = image.shape[0]\n",
        "    image_y = image.shape[1]\n",
        "    return image[size:image_x, size:image_y]\n",
        "\n",
        "def normalize_minmax(image_resized, batch_data, folder, idx):\n",
        "    batch_data[folder,idx,:,:,0] = cv2.normalize(image_resized[:,:,0], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    batch_data[folder,idx,:,:,1] = cv2.normalize(image_resized[:,:,1], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    batch_data[folder,idx,:,:,2] = cv2.normalize(image_resized[:,:,2], None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    return batch_data   \n",
        "\n",
        "def crop_and_normalize(image,batch_data, folder, idx, y=120,z=120):\n",
        "    \n",
        "    #crop the images and resize them. Note that the images are of 2 different shape \n",
        "    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "    image_cropped = crop_image(image)\n",
        "    image_resized = cv2.resize(image_cropped, (z,y), interpolation = cv2.INTER_AREA)\n",
        "                    \n",
        "    # using min max normalization.\n",
        "    pending_batch_data = normalize_minmax(image_resized, batch_data, folder, idx)\n",
        "    return pending_batch_data\n",
        "\n",
        "\n",
        "def create_batch_data(t, source_path, img_idx, folder, batch, batch_size, batch_data, batch_labels,dim=(120,120)):\n",
        "    imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    \n",
        "        image = cv2.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+\n",
        "                           imgs[item]).astype(np.float32)\n",
        "                    \n",
        "        # using min max normalization.\n",
        "        batch_data = crop_and_normalize(image, batch_data, folder, idx,y=dim[0],z=dim[1])\n",
        "                    \n",
        "    batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "    return batch_data, batch_labels\n",
        "\n",
        "def generator(source_path, folder_list, dim=(120,120), batch_size=30, ablation=None,samples=[5,10,15]):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size, 'dim=', dim)\n",
        "    x = len(samples) # number of taken from each video\n",
        "    img_idx = samples #create a list of image numbers you want to use for a particular video\n",
        "    y = dim[0] # image dim\n",
        "    z = dim[1] # image dim\n",
        "    while True:\n",
        "        if ablation is not None: \n",
        "            t = np.random.permutation(folder_list[:ablation])\n",
        "        else:\n",
        "            t = np.random.permutation(folder_list)\n",
        "        num_batches =  len(t) // batch_size # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            \n",
        "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size, 5)) # batch_labels is the one hot representation of the output\n",
        "            \n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                \n",
        "                ## get images from the folder\n",
        "                batch_data_labels = create_batch_data(t, source_path, img_idx, folder, batch, batch_size, batch_data, batch_labels,dim)\n",
        "                batch_data = batch_data_labels[0]\n",
        "                batch_labels = batch_data_labels[1]\n",
        "                \n",
        "            yield batch_data, batch_labels \n",
        "            \n",
        "        # The length of the folder list could leave some residue folders, the below code deals with it\n",
        "        # Number of pending batches\n",
        "        pending_batches = len(t) % batch_size\n",
        "        \n",
        "        pending_batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "        pending_batch_labels = np.zeros((batch_size, 5)) # batch_labels is the one hot representation of the output\n",
        "        \n",
        "        for folder in range(pending_batches): # iterate over the batch_size\n",
        "\n",
        "            # getimages from the folder\n",
        "            pending_batch_data_labels = create_batch_data(t, source_path, img_idx, folder, num_batches, batch_size, pending_batch_data, pending_batch_labels)\n",
        "            pending_batch_data = pending_batch_data_labels[0]\n",
        "            pending_batch_labels = pending_batch_data_labels[1]       \n",
        "            \n",
        "            \n",
        "        yield pending_batch_data, pending_batch_labels\n",
        "            \n",
        "\n",
        "# g = generator(data_dir_train, train_doc)\n",
        "# next(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urGjpChRRj9Q"
      },
      "source": [
        "# g = generator(data_dir_train, train_doc, (120,120), 30, ablation=None, samples=30)\n",
        "# p = 0\n",
        "# for k in g:\n",
        "#     p+=1\n",
        "# print(p)\n",
        "# # next(g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4jPv4PqRj9Q"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykbds8ttRj9Q"
      },
      "source": [
        "## Model\n",
        "\n",
        "#### Objective\n",
        "\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMPwJ1FOgDXP"
      },
      "source": [
        "### 3D Convolution\n",
        "\n",
        "We will build the model using 3D Convolution Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pv_E61DRj9R"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "#write your model here\n",
        "samples_per_image=list(range(0,30,2))\n",
        "\n",
        "# Building a 3 D Convolution model.\n",
        "model = Sequential()\n",
        "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(len(samples_per_image),120,120,3)))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Conv3D(64, kernel_size=(2, 2, 2), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wlQcIShRj9R"
      },
      "source": [
        "Now that you have written the model, the next step is to `compile` the model. When you print the `summary` of the model, you'll see the total number of parameters you have to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA-5dYSJRj9R",
        "scrolled": true,
        "outputId": "11612778-225a-44a4-ed7d-cabfe366c6e0"
      },
      "source": [
        "optimiser =  'sgd' #write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d (Conv3D)              (None, 13, 118, 118, 32)  2624      \n",
            "_________________________________________________________________\n",
            "max_pooling3d (MaxPooling3D) (None, 6, 59, 59, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_1 (Conv3D)            (None, 5, 58, 58, 64)     16448     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_1 (MaxPooling3 (None, 2, 29, 29, 64)     0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 107648)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               27558144  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 27,644,293\n",
            "Trainable params: 27,644,293\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLHuoMaGRj9R"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3LKQy6_fHrt"
      },
      "source": [
        "# Reusable function to run 3d convolution\n",
        "def run_3d_convolution(num_epochs, batch_size, dim=(120,120), ablation_size=None, callbacks_list=[]):\n",
        "  if ablation_size is None:\n",
        "    num_train_sequences = len(train_doc)\n",
        "    num_val_sequences = len(val_doc)\n",
        "  else:\n",
        "    num_train_sequences = len(train_doc[:ablation_size])\n",
        "    num_val_sequences = len(val_doc[:ablation_size])\n",
        "\n",
        "  print('# training sequences =', num_train_sequences)\n",
        "  print('# validation sequences =', num_val_sequences)\n",
        "  print ('# epochs =', num_epochs)\n",
        "\n",
        "  if (num_train_sequences%batch_size) == 0:\n",
        "     steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "  else:\n",
        "     steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "  if (num_val_sequences%batch_size) == 0:\n",
        "      validation_steps = int(num_val_sequences/batch_size)\n",
        "  else:\n",
        "      validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "  print('steps_per_epoch: {}, validation_steps: {}'.format(steps_per_epoch, validation_steps))\n",
        "\n",
        "  train_generator = generator(data_dir_train, train_doc, dim=dim, batch_size=batch_size, ablation=ablation_size, samples=samples_per_image)\n",
        "  val_generator = generator(data_dir_val, val_doc, dim=dim, batch_size=batch_size, ablation=ablation_size, samples=samples_per_image)\n",
        "\n",
        "  print('='*100)\n",
        "\n",
        "  model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrJonBC8Rj9S"
      },
      "source": [
        "### Ablation Experiment\n",
        "\n",
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`. \n",
        "We will fit the model with 1 epoch just to validate if the model is working.\n",
        "Notice I have not added any callbacks yet, we will do it in the later sections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUA57v_vRj9S",
        "outputId": "197116b0-9657-452f-8e77-f0e77170a0d1"
      },
      "source": [
        "# Experiment Parameters\n",
        "ablation_size=100\n",
        "num_epochs = 1\n",
        "batch_size=30\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 100\n",
            "# validation sequences = 100\n",
            "# epochs = 1\n",
            "steps_per_epoch: 4, validation_steps: 4\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "4/4 [==============================] - ETA: 0s - loss: 4.9564 - categorical_accuracy: 0.2764Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "4/4 [==============================] - 43s 3s/step - loss: 5.1041 - categorical_accuracy: 0.2644 - val_loss: 1.6216 - val_categorical_accuracy: 0.1917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_tbrRD9Rj9T"
      },
      "source": [
        "The model seems to be working well, there are no errors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VC6Oq9BRj9T"
      },
      "source": [
        "### Overfitting on training data. \n",
        "\n",
        "Let us overfit on the training data to see if the model is able to learn from the data. We are going to use less data and run for more epochs and see if the model is able to improve the accuracy and reduce the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JGbD1hXRj9T",
        "outputId": "4f883cb5-c25b-4ebe-bbab-ed604488b728"
      },
      "source": [
        "ablation_size=300\n",
        "batch_size=10\n",
        "num_epochs=10\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 300\n",
            "# validation sequences = 100\n",
            "# epochs = 10\n",
            "steps_per_epoch: 30, validation_steps: 10\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 10 dim= (120, 120)\n",
            "Epoch 1/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.5853 - categorical_accuracy: 0.2767Source path =  Project_data/val/ ; batch size = 10 dim= (120, 120)\n",
            "30/30 [==============================] - 15s 494ms/step - loss: 1.5853 - categorical_accuracy: 0.2767 - val_loss: 1.4875 - val_categorical_accuracy: 0.3600\n",
            "Epoch 2/10\n",
            "30/30 [==============================] - 13s 450ms/step - loss: 1.3208 - categorical_accuracy: 0.4400 - val_loss: 1.3518 - val_categorical_accuracy: 0.4300\n",
            "Epoch 3/10\n",
            "30/30 [==============================] - 13s 440ms/step - loss: 1.0519 - categorical_accuracy: 0.6000 - val_loss: 1.2726 - val_categorical_accuracy: 0.4900\n",
            "Epoch 4/10\n",
            "30/30 [==============================] - 13s 440ms/step - loss: 0.8156 - categorical_accuracy: 0.6800 - val_loss: 1.1767 - val_categorical_accuracy: 0.5100\n",
            "Epoch 5/10\n",
            "30/30 [==============================] - 13s 442ms/step - loss: 0.5478 - categorical_accuracy: 0.7933 - val_loss: 1.1730 - val_categorical_accuracy: 0.5900\n",
            "Epoch 6/10\n",
            "30/30 [==============================] - 13s 430ms/step - loss: 0.4821 - categorical_accuracy: 0.8167 - val_loss: 1.3969 - val_categorical_accuracy: 0.5500\n",
            "Epoch 7/10\n",
            "30/30 [==============================] - 13s 434ms/step - loss: 0.2358 - categorical_accuracy: 0.9000 - val_loss: 1.3799 - val_categorical_accuracy: 0.5600\n",
            "Epoch 8/10\n",
            "30/30 [==============================] - 13s 446ms/step - loss: 0.3113 - categorical_accuracy: 0.9000 - val_loss: 1.1239 - val_categorical_accuracy: 0.5800\n",
            "Epoch 9/10\n",
            "30/30 [==============================] - 13s 437ms/step - loss: 0.0563 - categorical_accuracy: 0.9667 - val_loss: 1.2231 - val_categorical_accuracy: 0.5700\n",
            "Epoch 10/10\n",
            "30/30 [==============================] - 13s 442ms/step - loss: 0.0301 - categorical_accuracy: 0.9667 - val_loss: 1.2177 - val_categorical_accuracy: 0.5900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFwy_8_vRj9T"
      },
      "source": [
        "The training loss reduced and accuracy increased, the model is able to learn well. There is significant gap between the training and validation accuracy so we have successfully overfit the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H35_C8B5Rj9U"
      },
      "source": [
        "### Callbacks\n",
        "\n",
        "Here we define few callbacks which we will use later when we fit the complete model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0Bi7jVuRj9U",
        "outputId": "a6234d63-d7ee-4006-dcaa-200c16c49553"
      },
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
        "\n",
        "ES = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "\n",
        "callbacks_list = [LR, ES]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhrN50BrRj9U"
      },
      "source": [
        "### Final Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raAKBxhnRj9U",
        "outputId": "cb2b315a-a6c0-47e2-a0fb-dff7ee61b2aa"
      },
      "source": [
        "#Experiment Parameters\n",
        "ablation_size=None\n",
        "batch_size=30\n",
        "num_epochs=20\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size,callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n",
            "steps_per_epoch: 23, validation_steps: 4\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "Epoch 1/20\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.9613 - categorical_accuracy: 0.6232Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "23/23 [==============================] - 28s 1s/step - loss: 0.9613 - categorical_accuracy: 0.6232 - val_loss: 0.8870 - val_categorical_accuracy: 0.4750\n",
            "Epoch 2/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.8479 - categorical_accuracy: 0.6754 - val_loss: 0.7713 - val_categorical_accuracy: 0.5750\n",
            "Epoch 3/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.7402 - categorical_accuracy: 0.6942 - val_loss: 0.7274 - val_categorical_accuracy: 0.5833\n",
            "Epoch 4/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.6314 - categorical_accuracy: 0.7478 - val_loss: 0.9686 - val_categorical_accuracy: 0.4500\n",
            "Epoch 5/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.5620 - categorical_accuracy: 0.7855 - val_loss: 0.7750 - val_categorical_accuracy: 0.5500\n",
            "Epoch 6/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.4047 - categorical_accuracy: 0.8580 - val_loss: 1.2510 - val_categorical_accuracy: 0.4583\n",
            "Epoch 7/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.2388 - categorical_accuracy: 0.9188 - val_loss: 0.5785 - val_categorical_accuracy: 0.6333\n",
            "Epoch 8/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1931 - categorical_accuracy: 0.9319 - val_loss: 0.7424 - val_categorical_accuracy: 0.5333\n",
            "Epoch 9/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1743 - categorical_accuracy: 0.9406 - val_loss: 0.7243 - val_categorical_accuracy: 0.5667\n",
            "Epoch 10/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.1619 - categorical_accuracy: 0.9420 - val_loss: 0.7869 - val_categorical_accuracy: 0.5333\n",
            "Epoch 11/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.1472 - categorical_accuracy: 0.9464 - val_loss: 0.7534 - val_categorical_accuracy: 0.6083\n",
            "Epoch 12/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1417 - categorical_accuracy: 0.9478 - val_loss: 0.7271 - val_categorical_accuracy: 0.5417\n",
            "Epoch 13/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1362 - categorical_accuracy: 0.9507 - val_loss: 0.7164 - val_categorical_accuracy: 0.5750\n",
            "Epoch 14/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1304 - categorical_accuracy: 0.9507 - val_loss: 0.5969 - val_categorical_accuracy: 0.5833\n",
            "Epoch 15/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1250 - categorical_accuracy: 0.9522 - val_loss: 0.7215 - val_categorical_accuracy: 0.5667\n",
            "Epoch 16/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1192 - categorical_accuracy: 0.9536 - val_loss: 0.7288 - val_categorical_accuracy: 0.5667\n",
            "Epoch 17/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.1149 - categorical_accuracy: 0.9551 - val_loss: 0.7187 - val_categorical_accuracy: 0.5750\n",
            "Epoch 18/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1118 - categorical_accuracy: 0.9551 - val_loss: 0.6967 - val_categorical_accuracy: 0.5583\n",
            "Epoch 19/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1071 - categorical_accuracy: 0.9551 - val_loss: 0.6823 - val_categorical_accuracy: 0.5833\n",
            "Epoch 20/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1031 - categorical_accuracy: 0.9551 - val_loss: 0.6527 - val_categorical_accuracy: 0.5833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYWbB0r5Rj9U",
        "outputId": "e5cb7fdd-02e5-4d1c-bf62-7bc00160b5ab"
      },
      "source": [
        "ablation_size=None\n",
        "batch_size=10\n",
        "num_epochs=20\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size,callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n",
            "steps_per_epoch: 67, validation_steps: 10\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 10 dim= (120, 120)\n",
            "Epoch 1/20\n",
            "67/67 [==============================] - ETA: 0s - loss: 0.1071 - categorical_accuracy: 0.9806Source path =  Project_data/val/ ; batch size = 10 dim= (120, 120)\n",
            "67/67 [==============================] - 25s 382ms/step - loss: 0.1071 - categorical_accuracy: 0.9806 - val_loss: 0.8996 - val_categorical_accuracy: 0.7000\n",
            "Epoch 2/20\n",
            "67/67 [==============================] - 25s 382ms/step - loss: 0.0960 - categorical_accuracy: 0.9866 - val_loss: 0.8741 - val_categorical_accuracy: 0.7100\n",
            "Epoch 3/20\n",
            "67/67 [==============================] - 26s 385ms/step - loss: 0.0849 - categorical_accuracy: 0.9896 - val_loss: 0.8924 - val_categorical_accuracy: 0.6900\n",
            "Epoch 4/20\n",
            "67/67 [==============================] - 26s 385ms/step - loss: 0.0778 - categorical_accuracy: 0.9881 - val_loss: 0.8832 - val_categorical_accuracy: 0.6800\n",
            "Epoch 5/20\n",
            "67/67 [==============================] - 25s 381ms/step - loss: 0.0695 - categorical_accuracy: 0.9896 - val_loss: 0.9115 - val_categorical_accuracy: 0.6700\n",
            "Epoch 6/20\n",
            "67/67 [==============================] - 25s 384ms/step - loss: 0.0630 - categorical_accuracy: 0.9896 - val_loss: 0.9084 - val_categorical_accuracy: 0.6900\n",
            "Epoch 7/20\n",
            "67/67 [==============================] - 25s 382ms/step - loss: 0.0577 - categorical_accuracy: 0.9896 - val_loss: 0.9272 - val_categorical_accuracy: 0.6800\n",
            "Epoch 8/20\n",
            "67/67 [==============================] - 25s 381ms/step - loss: 0.0529 - categorical_accuracy: 0.9896 - val_loss: 0.9146 - val_categorical_accuracy: 0.6800\n",
            "Epoch 9/20\n",
            "67/67 [==============================] - 26s 385ms/step - loss: 0.0491 - categorical_accuracy: 0.9896 - val_loss: 0.9249 - val_categorical_accuracy: 0.6900\n",
            "Epoch 10/20\n",
            "67/67 [==============================] - 25s 379ms/step - loss: 0.0458 - categorical_accuracy: 0.9896 - val_loss: 0.9264 - val_categorical_accuracy: 0.7100\n",
            "Epoch 11/20\n",
            "67/67 [==============================] - 25s 381ms/step - loss: 0.0418 - categorical_accuracy: 0.9896 - val_loss: 0.9368 - val_categorical_accuracy: 0.7200\n",
            "Epoch 12/20\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0386 - categorical_accuracy: 0.9896 - val_loss: 0.9592 - val_categorical_accuracy: 0.6800\n",
            "Epoch 13/20\n",
            "67/67 [==============================] - 25s 382ms/step - loss: 0.0368 - categorical_accuracy: 0.9896 - val_loss: 0.9528 - val_categorical_accuracy: 0.7000\n",
            "Epoch 14/20\n",
            "67/67 [==============================] - 25s 382ms/step - loss: 0.0339 - categorical_accuracy: 0.9896 - val_loss: 0.9583 - val_categorical_accuracy: 0.6700\n",
            "Epoch 15/20\n",
            "67/67 [==============================] - 25s 379ms/step - loss: 0.0316 - categorical_accuracy: 0.9896 - val_loss: 0.9662 - val_categorical_accuracy: 0.6800\n",
            "Epoch 16/20\n",
            "67/67 [==============================] - 25s 383ms/step - loss: 0.0301 - categorical_accuracy: 0.9896 - val_loss: 0.9562 - val_categorical_accuracy: 0.6800\n",
            "Epoch 17/20\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0283 - categorical_accuracy: 0.9896 - val_loss: 0.9789 - val_categorical_accuracy: 0.6900\n",
            "Epoch 18/20\n",
            "67/67 [==============================] - 25s 381ms/step - loss: 0.0264 - categorical_accuracy: 0.9896 - val_loss: 0.9772 - val_categorical_accuracy: 0.6700\n",
            "Epoch 19/20\n",
            "67/67 [==============================] - 25s 383ms/step - loss: 0.0249 - categorical_accuracy: 0.9896 - val_loss: 1.0009 - val_categorical_accuracy: 0.6700\n",
            "Epoch 20/20\n",
            "67/67 [==============================] - 25s 383ms/step - loss: 0.0237 - categorical_accuracy: 0.9896 - val_loss: 0.9909 - val_categorical_accuracy: 0.6800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwLTmyPktkaM"
      },
      "source": [
        "### Adding one more Conv Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnRBqugjtKuT",
        "outputId": "4bceabdb-bfae-46ef-b4af-20ccd1196f64"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "#write your model here\n",
        "samples_per_image=list(range(0,30,2))\n",
        "\n",
        "# Building a 3 D Convolution model.\n",
        "model = Sequential()\n",
        "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(len(samples_per_image),120, 120,3)))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Conv3D(64, kernel_size=(2, 2, 2), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "# learnings from https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/\n",
        "model.add(Conv3D(128, kernel_size=(1, 1, 1), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "optimiser =  'sgd' #write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_8 (Conv3D)            (None, 13, 118, 118, 32)  2624      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_8 (MaxPooling3 (None, 6, 59, 59, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_9 (Conv3D)            (None, 5, 58, 58, 64)     16448     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_9 (MaxPooling3 (None, 2, 29, 29, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_10 (Conv3D)           (None, 2, 29, 29, 128)    8320      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_10 (MaxPooling (None, 1, 14, 14, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 6,517,253\n",
            "Trainable params: 6,517,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQgJ5jojtf5P"
      },
      "source": [
        "### Overfitting the model intentionally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUIZz7watais",
        "outputId": "c5d5605a-0ced-46a9-8a46-ec08f1340847"
      },
      "source": [
        "ablation_size=300\n",
        "batch_size=30\n",
        "num_epochs=10\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 300\n",
            "# validation sequences = 100\n",
            "# epochs = 10\n",
            "steps_per_epoch: 10, validation_steps: 4\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.0108 - categorical_accuracy: 1.0000Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0108 - categorical_accuracy: 1.0000 - val_loss: 0.8441 - val_categorical_accuracy: 0.5167\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 14s 2s/step - loss: 0.0099 - categorical_accuracy: 0.9000 - val_loss: 0.8371 - val_categorical_accuracy: 0.5167\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0086 - categorical_accuracy: 0.9000 - val_loss: 0.8518 - val_categorical_accuracy: 0.5250\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0110 - categorical_accuracy: 0.9000 - val_loss: 0.7689 - val_categorical_accuracy: 0.5333\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0089 - categorical_accuracy: 0.9000 - val_loss: 0.8530 - val_categorical_accuracy: 0.5167\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0082 - categorical_accuracy: 0.9000 - val_loss: 0.7769 - val_categorical_accuracy: 0.5083\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0095 - categorical_accuracy: 0.9000 - val_loss: 0.9290 - val_categorical_accuracy: 0.5083\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0097 - categorical_accuracy: 0.9000 - val_loss: 0.8684 - val_categorical_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0082 - categorical_accuracy: 0.9000 - val_loss: 0.8529 - val_categorical_accuracy: 0.5167\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0086 - categorical_accuracy: 0.9000 - val_loss: 0.8881 - val_categorical_accuracy: 0.4833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUPxoS_E0ZKV"
      },
      "source": [
        "Training accuracy is around 90, and there is significant gap between training and validation accuracy, we have successfully overfit the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6HxUaHItqCo",
        "outputId": "955bf9aa-9205-4b2c-eca3-c1beab90fa76"
      },
      "source": [
        "#Experiment Parameters\n",
        "ablation_size=None\n",
        "batch_size=10\n",
        "num_epochs=30\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size,callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 30\n",
            "steps_per_epoch: 67, validation_steps: 10\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 10 dim= (120, 120)\n",
            "Epoch 1/30\n",
            "67/67 [==============================] - ETA: 0s - loss: 2.2473 - categorical_accuracy: 0.2525Source path =  Project_data/val/ ; batch size = 10 dim= (120, 120)\n",
            "67/67 [==============================] - 26s 382ms/step - loss: 2.2399 - categorical_accuracy: 0.2527 - val_loss: 1.5012 - val_categorical_accuracy: 0.1900\n",
            "Epoch 2/30\n",
            "67/67 [==============================] - 25s 377ms/step - loss: 1.4672 - categorical_accuracy: 0.3392 - val_loss: 1.3248 - val_categorical_accuracy: 0.4900\n",
            "Epoch 3/30\n",
            "67/67 [==============================] - 25s 381ms/step - loss: 1.2450 - categorical_accuracy: 0.5313 - val_loss: 1.2403 - val_categorical_accuracy: 0.4900\n",
            "Epoch 4/30\n",
            "67/67 [==============================] - 25s 376ms/step - loss: 1.1073 - categorical_accuracy: 0.5933 - val_loss: 1.2571 - val_categorical_accuracy: 0.5100\n",
            "Epoch 5/30\n",
            "67/67 [==============================] - 25s 379ms/step - loss: 0.9722 - categorical_accuracy: 0.6020 - val_loss: 1.2188 - val_categorical_accuracy: 0.4500\n",
            "Epoch 6/30\n",
            "67/67 [==============================] - 25s 375ms/step - loss: 0.7791 - categorical_accuracy: 0.7118 - val_loss: 1.1128 - val_categorical_accuracy: 0.5100\n",
            "Epoch 7/30\n",
            "67/67 [==============================] - 25s 377ms/step - loss: 0.6496 - categorical_accuracy: 0.7614 - val_loss: 0.9164 - val_categorical_accuracy: 0.6300\n",
            "Epoch 8/30\n",
            "67/67 [==============================] - 25s 375ms/step - loss: 0.4188 - categorical_accuracy: 0.8697 - val_loss: 1.1507 - val_categorical_accuracy: 0.5300\n",
            "Epoch 9/30\n",
            "67/67 [==============================] - 25s 380ms/step - loss: 0.3301 - categorical_accuracy: 0.9093 - val_loss: 1.0756 - val_categorical_accuracy: 0.6300\n",
            "Epoch 10/30\n",
            "67/67 [==============================] - 25s 372ms/step - loss: 0.2037 - categorical_accuracy: 0.9613 - val_loss: 1.0269 - val_categorical_accuracy: 0.6400\n",
            "Epoch 11/30\n",
            "67/67 [==============================] - 25s 376ms/step - loss: 0.1106 - categorical_accuracy: 0.9844 - val_loss: 0.9780 - val_categorical_accuracy: 0.6300\n",
            "Epoch 12/30\n",
            "67/67 [==============================] - 25s 377ms/step - loss: 0.0674 - categorical_accuracy: 0.9945 - val_loss: 1.0022 - val_categorical_accuracy: 0.6800\n",
            "Epoch 13/30\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0624 - categorical_accuracy: 0.9981 - val_loss: 1.0345 - val_categorical_accuracy: 0.6600\n",
            "Epoch 14/30\n",
            "67/67 [==============================] - 25s 371ms/step - loss: 0.0544 - categorical_accuracy: 0.9997 - val_loss: 1.0324 - val_categorical_accuracy: 0.6600\n",
            "Epoch 15/30\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0453 - categorical_accuracy: 0.9997 - val_loss: 1.0278 - val_categorical_accuracy: 0.6600\n",
            "Epoch 16/30\n",
            "67/67 [==============================] - 25s 375ms/step - loss: 0.0469 - categorical_accuracy: 0.9997 - val_loss: 1.0383 - val_categorical_accuracy: 0.6100\n",
            "Epoch 17/30\n",
            "67/67 [==============================] - 25s 380ms/step - loss: 0.0418 - categorical_accuracy: 0.9997 - val_loss: 1.0557 - val_categorical_accuracy: 0.6500\n",
            "Epoch 18/30\n",
            "67/67 [==============================] - 25s 376ms/step - loss: 0.0391 - categorical_accuracy: 0.9997 - val_loss: 1.0584 - val_categorical_accuracy: 0.6500\n",
            "Epoch 19/30\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0373 - categorical_accuracy: 0.9997 - val_loss: 1.0556 - val_categorical_accuracy: 0.6500\n",
            "Epoch 20/30\n",
            "67/67 [==============================] - 25s 374ms/step - loss: 0.0364 - categorical_accuracy: 0.9997 - val_loss: 1.0568 - val_categorical_accuracy: 0.6500\n",
            "Epoch 21/30\n",
            "67/67 [==============================] - 25s 379ms/step - loss: 0.0363 - categorical_accuracy: 0.9997 - val_loss: 1.0603 - val_categorical_accuracy: 0.6500\n",
            "Epoch 22/30\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0329 - categorical_accuracy: 0.9997 - val_loss: 1.0604 - val_categorical_accuracy: 0.6700\n",
            "Epoch 23/30\n",
            "67/67 [==============================] - 25s 376ms/step - loss: 0.0353 - categorical_accuracy: 0.9997 - val_loss: 1.0748 - val_categorical_accuracy: 0.6500\n",
            "Epoch 24/30\n",
            "67/67 [==============================] - 25s 375ms/step - loss: 0.0343 - categorical_accuracy: 0.9997 - val_loss: 1.0921 - val_categorical_accuracy: 0.6500\n",
            "Epoch 25/30\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0280 - categorical_accuracy: 0.9997 - val_loss: 1.0884 - val_categorical_accuracy: 0.6600\n",
            "Epoch 26/30\n",
            "67/67 [==============================] - 25s 376ms/step - loss: 0.0281 - categorical_accuracy: 0.9997 - val_loss: 1.0756 - val_categorical_accuracy: 0.6600\n",
            "Epoch 27/30\n",
            "67/67 [==============================] - 25s 375ms/step - loss: 0.0284 - categorical_accuracy: 0.9997 - val_loss: 1.0989 - val_categorical_accuracy: 0.6600\n",
            "Epoch 28/30\n",
            "67/67 [==============================] - 25s 375ms/step - loss: 0.0264 - categorical_accuracy: 0.9997 - val_loss: 1.0942 - val_categorical_accuracy: 0.6700\n",
            "Epoch 29/30\n",
            "67/67 [==============================] - 25s 378ms/step - loss: 0.0267 - categorical_accuracy: 0.9997 - val_loss: 1.0949 - val_categorical_accuracy: 0.6700\n",
            "Epoch 30/30\n",
            "67/67 [==============================] - 25s 373ms/step - loss: 0.0235 - categorical_accuracy: 0.9997 - val_loss: 1.1140 - val_categorical_accuracy: 0.6600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CakL4ZYDtqX4",
        "outputId": "f5049f59-f50b-4e29-d781-ed02afacfbd2"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "#write your model here\n",
        "samples_per_image=list(range(0,30,2))\n",
        "\n",
        "# Building a 3 D Convolution model.\n",
        "model = Sequential()\n",
        "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(len(samples_per_image),120, 120,3)))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(BatchNormalization()),\n",
        "\n",
        "model.add(Conv3D(64, kernel_size=(2, 2, 2), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(BatchNormalization()),\n",
        "\n",
        "# learnings from https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/\n",
        "model.add(Conv3D(128, kernel_size=(1, 1, 1), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(BatchNormalization()),\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "optimiser =  'sgd' #write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_14 (Conv3D)           (None, 13, 118, 118, 32)  2624      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_14 (MaxPooling (None, 6, 59, 59, 32)     0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 6, 59, 59, 32)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 6, 59, 59, 32)     128       \n",
            "_________________________________________________________________\n",
            "conv3d_15 (Conv3D)           (None, 5, 58, 58, 64)     16448     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_15 (MaxPooling (None, 2, 29, 29, 64)     0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2, 29, 29, 64)     0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 2, 29, 29, 64)     256       \n",
            "_________________________________________________________________\n",
            "conv3d_16 (Conv3D)           (None, 2, 29, 29, 128)    8320      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_16 (MaxPooling (None, 1, 14, 14, 128)    0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1, 14, 14, 128)    0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1, 14, 14, 128)    512       \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 6,518,149\n",
            "Trainable params: 6,517,701\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRAcmzL49naf"
      },
      "source": [
        "### Ablation Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5G4motUb9n0f",
        "outputId": "2c799931-033e-496d-a197-809e397fcb53"
      },
      "source": [
        "# Experiment Parameters\n",
        "ablation_size=100\n",
        "num_epochs = 1\n",
        "batch_size=30\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 100\n",
            "# validation sequences = 100\n",
            "# epochs = 1\n",
            "steps_per_epoch: 4, validation_steps: 4\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "4/4 [==============================] - ETA: 0s - loss: 2.6028 - categorical_accuracy: 0.1597Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "4/4 [==============================] - 9s 3s/step - loss: 2.5212 - categorical_accuracy: 0.1611 - val_loss: 1.5685 - val_categorical_accuracy: 0.2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZtE0WO09oa8"
      },
      "source": [
        "### Overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgeVOEE_9ow-",
        "outputId": "6a74947d-7dad-46e7-e5be-633a68ac71e6"
      },
      "source": [
        "ablation_size=300\n",
        "batch_size=10\n",
        "num_epochs=10\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 300\n",
            "# validation sequences = 100\n",
            "# epochs = 10\n",
            "steps_per_epoch: 30, validation_steps: 10\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 10 dim= (120, 120)\n",
            "Epoch 1/10\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.5504 - categorical_accuracy: 0.4367Source path =  Project_data/val/ ; batch size = 10 dim= (120, 120)\n",
            "30/30 [==============================] - 13s 447ms/step - loss: 1.5504 - categorical_accuracy: 0.4367 - val_loss: 1.8248 - val_categorical_accuracy: 0.3700\n",
            "Epoch 2/10\n",
            "30/30 [==============================] - 13s 448ms/step - loss: 0.8286 - categorical_accuracy: 0.6767 - val_loss: 3.7559 - val_categorical_accuracy: 0.2200\n",
            "Epoch 3/10\n",
            "30/30 [==============================] - 13s 436ms/step - loss: 0.6084 - categorical_accuracy: 0.7567 - val_loss: 4.5915 - val_categorical_accuracy: 0.2100\n",
            "Epoch 4/10\n",
            "30/30 [==============================] - 13s 448ms/step - loss: 0.4099 - categorical_accuracy: 0.8233 - val_loss: 7.4317 - val_categorical_accuracy: 0.2100\n",
            "Epoch 5/10\n",
            "30/30 [==============================] - 13s 441ms/step - loss: 0.3720 - categorical_accuracy: 0.8433 - val_loss: 5.8541 - val_categorical_accuracy: 0.2700\n",
            "Epoch 6/10\n",
            "30/30 [==============================] - 13s 435ms/step - loss: 0.2454 - categorical_accuracy: 0.8867 - val_loss: 6.9659 - val_categorical_accuracy: 0.2000\n",
            "Epoch 7/10\n",
            "30/30 [==============================] - 13s 453ms/step - loss: 0.0980 - categorical_accuracy: 0.9500 - val_loss: 7.8473 - val_categorical_accuracy: 0.2100\n",
            "Epoch 8/10\n",
            "30/30 [==============================] - 13s 442ms/step - loss: 0.1433 - categorical_accuracy: 0.9400 - val_loss: 7.5162 - val_categorical_accuracy: 0.2200\n",
            "Epoch 9/10\n",
            "30/30 [==============================] - 13s 437ms/step - loss: 0.0734 - categorical_accuracy: 0.9467 - val_loss: 7.7193 - val_categorical_accuracy: 0.2200\n",
            "Epoch 10/10\n",
            "30/30 [==============================] - 13s 437ms/step - loss: 0.1136 - categorical_accuracy: 0.9333 - val_loss: 7.5037 - val_categorical_accuracy: 0.2100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zpfomdU9pQs"
      },
      "source": [
        "### Final Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCcSituM9pbw",
        "outputId": "cd8e1191-8f87-4f03-ce8a-aa05ec95ae9e"
      },
      "source": [
        "#Experiment Parameters\n",
        "ablation_size=None\n",
        "batch_size=30\n",
        "num_epochs=30\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size,callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 30\n",
            "steps_per_epoch: 23, validation_steps: 4\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "Epoch 1/30\n",
            "23/23 [==============================] - ETA: 0s - loss: 0.1553 - categorical_accuracy: 0.9174Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "23/23 [==============================] - 27s 1s/step - loss: 0.1553 - categorical_accuracy: 0.9174 - val_loss: 1.3541 - val_categorical_accuracy: 0.4417\n",
            "Epoch 2/30\n",
            "23/23 [==============================] - 29s 1s/step - loss: 0.1312 - categorical_accuracy: 0.9130 - val_loss: 1.4157 - val_categorical_accuracy: 0.6083\n",
            "Epoch 3/30\n",
            "23/23 [==============================] - 27s 1s/step - loss: 0.2113 - categorical_accuracy: 0.9217 - val_loss: 2.5982 - val_categorical_accuracy: 0.2917\n",
            "Epoch 4/30\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.1359 - categorical_accuracy: 0.9319 - val_loss: 1.1372 - val_categorical_accuracy: 0.4417\n",
            "Epoch 5/30\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.1794 - categorical_accuracy: 0.9130 - val_loss: 3.2580 - val_categorical_accuracy: 0.3083\n",
            "Epoch 6/30\n",
            "23/23 [==============================] - 27s 1s/step - loss: 0.2260 - categorical_accuracy: 0.9087 - val_loss: 2.4798 - val_categorical_accuracy: 0.3500\n",
            "Epoch 7/30\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.2945 - categorical_accuracy: 0.8841 - val_loss: 3.0387 - val_categorical_accuracy: 0.4750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lURx93kyDMTe",
        "outputId": "4e661453-56ce-4255-eb5a-18b5f339225a"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
        "from keras.layers.convolutional import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "#write your model here\n",
        "samples_per_image=list(range(0,30,2))\n",
        "\n",
        "# Building a 3 D Convolution model.\n",
        "model = Sequential()\n",
        "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(len(samples_per_image),120, 120,3)))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(Dropout(0.25)),\n",
        "\n",
        "model.add(Conv3D(64, kernel_size=(2, 2, 2), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(Dropout(0.25)),\n",
        "\n",
        "# learnings from https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/\n",
        "model.add(Conv3D(128, kernel_size=(1, 1, 1), activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dropout(0.25)),\n",
        "model.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "optimiser =  'sgd' #write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv3d_17 (Conv3D)           (None, 13, 118, 118, 32)  2624      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_17 (MaxPooling (None, 6, 59, 59, 32)     0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 6, 59, 59, 32)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_18 (Conv3D)           (None, 5, 58, 58, 64)     16448     \n",
            "_________________________________________________________________\n",
            "max_pooling3d_18 (MaxPooling (None, 2, 29, 29, 64)     0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 2, 29, 29, 64)     0         \n",
            "_________________________________________________________________\n",
            "conv3d_19 (Conv3D)           (None, 2, 29, 29, 128)    8320      \n",
            "_________________________________________________________________\n",
            "max_pooling3d_19 (MaxPooling (None, 1, 14, 14, 128)    0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 6,517,253\n",
            "Trainable params: 6,517,253\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKhNM5VpDTdu",
        "outputId": "db156db3-e082-4e1d-e2d7-768d789ab988"
      },
      "source": [
        "#Experiment Parameters\n",
        "ablation_size=None\n",
        "batch_size=30\n",
        "num_epochs=20\n",
        "dim=(120,120)\n",
        "\n",
        "run_3d_convolution(num_epochs, batch_size, dim, ablation_size,callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n",
            "steps_per_epoch: 23, validation_steps: 4\n",
            "====================================================================================================\n",
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "Epoch 1/20\n",
            "23/23 [==============================] - ETA: 0s - loss: 4.8133 - categorical_accuracy: 0.1934Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "23/23 [==============================] - 28s 1s/step - loss: 4.7297 - categorical_accuracy: 0.1932 - val_loss: 1.3013 - val_categorical_accuracy: 0.2417\n",
            "Epoch 2/20\n",
            "23/23 [==============================] - 33s 2s/step - loss: 1.5556 - categorical_accuracy: 0.2547 - val_loss: 1.2635 - val_categorical_accuracy: 0.2083\n",
            "Epoch 3/20\n",
            "23/23 [==============================] - 30s 1s/step - loss: 1.5231 - categorical_accuracy: 0.2497 - val_loss: 1.2793 - val_categorical_accuracy: 0.1417\n",
            "Epoch 4/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 1.4919 - categorical_accuracy: 0.3101 - val_loss: 1.2280 - val_categorical_accuracy: 0.3083\n",
            "Epoch 5/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.4681 - categorical_accuracy: 0.3131 - val_loss: 1.2240 - val_categorical_accuracy: 0.3250\n",
            "Epoch 6/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.4194 - categorical_accuracy: 0.4001 - val_loss: 1.2039 - val_categorical_accuracy: 0.4000\n",
            "Epoch 7/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.4067 - categorical_accuracy: 0.3977 - val_loss: 1.1700 - val_categorical_accuracy: 0.4083\n",
            "Epoch 8/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 1.3131 - categorical_accuracy: 0.4284 - val_loss: 1.1618 - val_categorical_accuracy: 0.4583\n",
            "Epoch 9/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.3043 - categorical_accuracy: 0.4813 - val_loss: 1.1404 - val_categorical_accuracy: 0.4083\n",
            "Epoch 10/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 1.2104 - categorical_accuracy: 0.5205 - val_loss: 1.0942 - val_categorical_accuracy: 0.4750\n",
            "Epoch 11/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.1997 - categorical_accuracy: 0.5446 - val_loss: 1.0915 - val_categorical_accuracy: 0.3833\n",
            "Epoch 12/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 1.1094 - categorical_accuracy: 0.5900 - val_loss: 1.0409 - val_categorical_accuracy: 0.4417\n",
            "Epoch 13/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.1239 - categorical_accuracy: 0.6031 - val_loss: 1.0841 - val_categorical_accuracy: 0.4083\n",
            "Epoch 14/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.0298 - categorical_accuracy: 0.6398 - val_loss: 1.0702 - val_categorical_accuracy: 0.4333\n",
            "Epoch 15/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 1.0114 - categorical_accuracy: 0.6298 - val_loss: 1.0124 - val_categorical_accuracy: 0.4167\n",
            "Epoch 16/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.9628 - categorical_accuracy: 0.6435 - val_loss: 0.9565 - val_categorical_accuracy: 0.4500\n",
            "Epoch 17/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.9073 - categorical_accuracy: 0.6755 - val_loss: 1.1447 - val_categorical_accuracy: 0.3417\n",
            "Epoch 18/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.8309 - categorical_accuracy: 0.7051 - val_loss: 1.1219 - val_categorical_accuracy: 0.3583\n",
            "Epoch 19/20\n",
            "23/23 [==============================] - 26s 1s/step - loss: 0.9062 - categorical_accuracy: 0.6687 - val_loss: 0.9450 - val_categorical_accuracy: 0.4750\n",
            "Epoch 20/20\n",
            "23/23 [==============================] - 25s 1s/step - loss: 0.7654 - categorical_accuracy: 0.7029 - val_loss: 0.8917 - val_categorical_accuracy: 0.4917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bft5Hk-E8fDJ"
      },
      "source": [
        "### CNN + RNN Model using Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_BFERJIJIgv"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Flatten,Dense,TimeDistributed,GRU,Conv2D,MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfRgr7lWhOMw",
        "outputId": "48a2fd53-922c-4bd9-9410-d7dcd24ba977"
      },
      "source": [
        "resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(120,120,3))\n",
        "for layer in resnet.layers[:-10]:\n",
        "    layer.trainable=False\n",
        "\n",
        "cnn = Sequential([resnet])\n",
        "cnn.add(Conv2D(16, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(120,120,3)))\n",
        "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Conv2D(32, kernel_size=(2, 2), activation='relu', kernel_initializer='he_uniform'))\n",
        "# cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "cnn.add(Dense(256, activation='relu', kernel_initializer='he_uniform'))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(cnn,input_shape=(10,120,120,3)))\n",
        "model.add(GRU(16,input_shape=(None,30,256),return_sequences=True))\n",
        "model.add(GRU(8))\n",
        "model.add(Dense(5,activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwpwB-T1Ve-c",
        "outputId": "67c8fad4-78d6-45d0-ce59-d567c8b2d163"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed (TimeDistri (None, 10, 256)           23958960  \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 10, 16)            13152     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 8)                 624       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 45        \n",
            "=================================================================\n",
            "Total params: 23,972,781\n",
            "Trainable params: 4,850,733\n",
            "Non-trainable params: 19,122,048\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQmAAGVVSYo3",
        "outputId": "5cda1316-dd11-4399-e8fc-00a533c344c8"
      },
      "source": [
        "optimiser =  'sgd' #write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "time_distributed (TimeDistri (None, 10, 256)           23958960  \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (None, 10, 16)            13152     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, 8)                 624       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 45        \n",
            "=================================================================\n",
            "Total params: 23,972,781\n",
            "Trainable params: 4,850,733\n",
            "Non-trainable params: 19,122,048\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNwVbp5N-8Ta"
      },
      "source": [
        "### Ablation Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lmQc41OQOiF",
        "outputId": "dc697589-ae26-4d2c-ca80-ed00bb87c768"
      },
      "source": [
        "ablation_size=100\n",
        "batch_size=30\n",
        "num_epochs=1\n",
        "\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "print ('# epochs =', num_epochs)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print('steps_per_epoch: {}, validation_steps: {}'.format(steps_per_epoch, validation_steps))\n",
        "\n",
        "train_generator = generator(data_dir_train, train_doc, dim=(120,120), batch_size=batch_size, ablation=ablation_size, samples=list(range(0,30,3)))\n",
        "val_generator = generator(data_dir_val, val_doc, dim=(120,120), batch_size=batch_size, ablation=ablation_size, samples=list(range(0,30,3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 1\n",
            "steps_per_epoch: 23, validation_steps: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX3ThyUYhxgQ",
        "outputId": "db3926a3-8518-4034-9cf8-9ca81f17d427"
      },
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=[], validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.3877 - accuracy: 0.2791Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "23/23 [==============================] - 33s 973ms/step - loss: 1.3870 - accuracy: 0.2782 - val_loss: 1.4787 - val_accuracy: 0.3167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7febcc5f2790>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9ReFtMT_QA8"
      },
      "source": [
        "## Overfitting Intentionally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e1DG8sy_RmA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc3fb57-22ed-45f9-89aa-20251e35c607"
      },
      "source": [
        "ablation_size=None\n",
        "batch_size=30\n",
        "num_epochs=10\n",
        "\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "print ('# epochs =', num_epochs)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print('steps_per_epoch: {}, validation_steps: {}'.format(steps_per_epoch, validation_steps))\n",
        "\n",
        "train_generator = generator(data_dir_train, train_doc, dim=(120,120), batch_size=batch_size, ablation=ablation_size, samples=list(range(0,30,3)))\n",
        "val_generator = generator(data_dir_val, val_doc, dim=(120,120), batch_size=batch_size, ablation=ablation_size, samples=list(range(0,30,3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 10\n",
            "steps_per_epoch: 23, validation_steps: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdTUmOd4Q7hn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df48d24-2853-4b74-b7b1-2098de657611"
      },
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=[], validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "Epoch 1/10\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.5143 - accuracy: 0.2710Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "23/23 [==============================] - 22s 956ms/step - loss: 1.5143 - accuracy: 0.2710 - val_loss: 1.3278 - val_accuracy: 0.1917\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 21s 929ms/step - loss: 1.4501 - accuracy: 0.3290 - val_loss: 1.3306 - val_accuracy: 0.2250\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 21s 931ms/step - loss: 1.4263 - accuracy: 0.3362 - val_loss: 1.3654 - val_accuracy: 0.1833\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 21s 914ms/step - loss: 1.3738 - accuracy: 0.3507 - val_loss: 1.3854 - val_accuracy: 0.1833\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 21s 915ms/step - loss: 1.3559 - accuracy: 0.3870 - val_loss: 1.3315 - val_accuracy: 0.1917\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 21s 932ms/step - loss: 1.3145 - accuracy: 0.3986 - val_loss: 1.2358 - val_accuracy: 0.2833\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 21s 927ms/step - loss: 1.3093 - accuracy: 0.4072 - val_loss: 1.4330 - val_accuracy: 0.2000\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 20s 898ms/step - loss: 1.2586 - accuracy: 0.4391 - val_loss: 1.2139 - val_accuracy: 0.2750\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 21s 925ms/step - loss: 1.2501 - accuracy: 0.4348 - val_loss: 1.3397 - val_accuracy: 0.2417\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 21s 922ms/step - loss: 1.2293 - accuracy: 0.4638 - val_loss: 1.4003 - val_accuracy: 0.2417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feb641e2bd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdoD83f9_aW8"
      },
      "source": [
        "## Final Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKuzEWuK_gRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd6b169b-ea31-4d08-86f8-04b756c272bf"
      },
      "source": [
        "ablation_size=None\n",
        "batch_size=30\n",
        "num_epochs=20\n",
        "\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "print ('# epochs =', num_epochs)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print('steps_per_epoch: {}, validation_steps: {}'.format(steps_per_epoch, validation_steps))\n",
        "\n",
        "train_generator = generator(data_dir_train, train_doc, dim=(120,120), batch_size=batch_size, ablation=ablation_size, samples=list(range(0,30,3)))\n",
        "val_generator = generator(data_dir_val, val_doc, dim=(120,120), batch_size=batch_size, ablation=ablation_size, samples=list(range(0,30,3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 20\n",
            "steps_per_epoch: 23, validation_steps: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw5UtoP__iel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531567fe-9f56-4324-d4d7-6c9c3810de6e"
      },
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=[], validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source path =  Project_data/train/ ; batch size = 30 dim= (120, 120)\n",
            "Epoch 1/20\n",
            "23/23 [==============================] - ETA: 0s - loss: 1.2837 - accuracy: 0.4058Source path =  Project_data/val/ ; batch size = 30 dim= (120, 120)\n",
            "23/23 [==============================] - 21s 939ms/step - loss: 1.2837 - accuracy: 0.4058 - val_loss: 1.3758 - val_accuracy: 0.2250\n",
            "Epoch 2/20\n",
            "23/23 [==============================] - 21s 930ms/step - loss: 1.1789 - accuracy: 0.4725 - val_loss: 1.3594 - val_accuracy: 0.2167\n",
            "Epoch 3/20\n",
            "23/23 [==============================] - 21s 916ms/step - loss: 1.1700 - accuracy: 0.4812 - val_loss: 1.1822 - val_accuracy: 0.3333\n",
            "Epoch 4/20\n",
            "23/23 [==============================] - 21s 906ms/step - loss: 1.1766 - accuracy: 0.5362 - val_loss: 1.2202 - val_accuracy: 0.3167\n",
            "Epoch 5/20\n",
            "23/23 [==============================] - 21s 933ms/step - loss: 1.1436 - accuracy: 0.5594 - val_loss: 1.2143 - val_accuracy: 0.3583\n",
            "Epoch 6/20\n",
            "23/23 [==============================] - 20s 903ms/step - loss: 1.1149 - accuracy: 0.5551 - val_loss: 1.2266 - val_accuracy: 0.3000\n",
            "Epoch 7/20\n",
            "23/23 [==============================] - 21s 931ms/step - loss: 1.2604 - accuracy: 0.4551 - val_loss: 1.5654 - val_accuracy: 0.1750\n",
            "Epoch 8/20\n",
            "23/23 [==============================] - 21s 916ms/step - loss: 1.3690 - accuracy: 0.3812 - val_loss: 1.5268 - val_accuracy: 0.2000\n",
            "Epoch 9/20\n",
            "23/23 [==============================] - 21s 929ms/step - loss: 1.2056 - accuracy: 0.4768 - val_loss: 1.4602 - val_accuracy: 0.2250\n",
            "Epoch 10/20\n",
            "23/23 [==============================] - 21s 928ms/step - loss: 1.1426 - accuracy: 0.4971 - val_loss: 1.2688 - val_accuracy: 0.2833\n",
            "Epoch 11/20\n",
            "23/23 [==============================] - 21s 926ms/step - loss: 1.1116 - accuracy: 0.5855 - val_loss: 1.5071 - val_accuracy: 0.2333\n",
            "Epoch 12/20\n",
            "23/23 [==============================] - 21s 906ms/step - loss: 1.0671 - accuracy: 0.5783 - val_loss: 1.1895 - val_accuracy: 0.3583\n",
            "Epoch 13/20\n",
            "23/23 [==============================] - 21s 910ms/step - loss: 1.0753 - accuracy: 0.5739 - val_loss: 1.3885 - val_accuracy: 0.2667\n",
            "Epoch 14/20\n",
            "23/23 [==============================] - 21s 919ms/step - loss: 1.0269 - accuracy: 0.6043 - val_loss: 1.0773 - val_accuracy: 0.4000\n",
            "Epoch 15/20\n",
            "23/23 [==============================] - 21s 930ms/step - loss: 1.0183 - accuracy: 0.6101 - val_loss: 1.1983 - val_accuracy: 0.2750\n",
            "Epoch 16/20\n",
            "23/23 [==============================] - 20s 900ms/step - loss: 1.2789 - accuracy: 0.4435 - val_loss: 1.1161 - val_accuracy: 0.3417\n",
            "Epoch 17/20\n",
            "23/23 [==============================] - 21s 929ms/step - loss: 1.0901 - accuracy: 0.5942 - val_loss: 1.3241 - val_accuracy: 0.2500\n",
            "Epoch 18/20\n",
            "23/23 [==============================] - 21s 913ms/step - loss: 1.1614 - accuracy: 0.4739 - val_loss: 1.1718 - val_accuracy: 0.3667\n",
            "Epoch 19/20\n",
            "23/23 [==============================] - 21s 925ms/step - loss: 1.0130 - accuracy: 0.5768 - val_loss: 1.1609 - val_accuracy: 0.3667\n",
            "Epoch 20/20\n",
            "23/23 [==============================] - 21s 908ms/step - loss: 0.9639 - accuracy: 0.6348 - val_loss: 1.1738 - val_accuracy: 0.3417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7feb06382650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    }
  ]
}